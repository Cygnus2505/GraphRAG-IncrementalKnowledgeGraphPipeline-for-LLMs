# GraphRAG Flink Pipeline Configuration

# ============================================================================
# Flink Configuration
# ============================================================================
flink {
  parallelism = 4
  checkpoint-interval-ms = 60000
  
  # Input source
  input {
    path = "phase1-delta-export/data/chunks.jsonl"
    format = "jsonl"
  }
}

# ============================================================================
# Ollama LLM Configuration
# ============================================================================
ollama {
  # For WSL: Use Windows host IP (same as Neo4j)
  endpoint = "http://10.0.0.201:11434"
  model = "tinyllama:latest"  # Using your installed model (faster, smaller)
  # Alternative: Download llama3 for better quality: ollama pull llama3
  temperature = 0.0
  timeout-ms = 60000  # Increased to 60 seconds for tinyllama
  max-retries = 3
}

# ============================================================================
# Neo4j Configuration (Neo4j Aura)
# ============================================================================
neo4j {
  # Neo4j Aura connection (secure)
  # Override with NEO4J_URI environment variable if needed
  uri = "neo4j+s://e86ce959.databases.neo4j.io"  # Neo4j Aura secure connection
  user = "neo4j"
  password = "cMGY0uqeoqpi1PMEnT6zrxrXQw6Cx42iyd-ZseuODGI"  # Aura password
  database = "neo4j"  # Aura uses "neo4j" as default database
  batch-size = 200
  max-retries = 8
}

# ============================================================================
# Relation Extraction Configuration
# ============================================================================
relation {
  cooccur {
    window = 3
    min-pmi = 0.2
  }
  
  llm {
    predicate-set = ["is_a", "part_of", "causes", "synonym_of", "related_to"]
    min-confidence = 0.65
  }
}

# ============================================================================
# Concept Extraction Configuration
# ============================================================================
concept {
  min-length = 2
  max-length = 50
  
  # Enable/disable extraction strategies
  strategies {
    heuristic = true
    nlp = false  # Stanford CoreNLP (resource-intensive)
    llm = false  # LLM-based extraction (expensive)
  }
}













